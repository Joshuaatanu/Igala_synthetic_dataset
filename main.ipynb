{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Igala                  English\n",
      "0            òfo ómẹ̀nyí             uncle's zero\n",
      "1                 nù ájì              pond's play\n",
      "2               ùmà ógbó  old age's comprehension\n",
      "3  ẹ́kọ́-àbọ́-ìlẹ̀ òbàtà   punishment's sociology\n",
      "4      yégéyégé ọ̀gẹ́cha            sincere's ice\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"igala_updated_with_pos.csv\")\n",
    "\n",
    "# --- Helper Function to Get Random Words of a Specific POS ---\n",
    "def get_random_word(df, pos_tag):\n",
    "  \"\"\"Gets a random word from the DataFrame matching the given POS tag.\"\"\"\n",
    "  words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "  if words:\n",
    "    return random.choice(words)\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "# --- Generate Synthetic Data ---\n",
    "num_samples = 1000 # Control how many phrases you want to generate\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    noun1 = get_random_word(data, r\"\\bNN\\b\") # Match NN exactly, not other tags like NNS\n",
    "    noun2 = get_random_word(data, r\"\\bNN\\b\")\n",
    "\n",
    "    # Ensure we found valid nouns\n",
    "    if noun1 and noun2:\n",
    "        igala_phrase = f\"{noun2} {noun1}\"\n",
    "        english_phrase = f\"{data['English'][data['Igala'] == noun1].iloc[0]}'s {data['English'][data['Igala'] == noun2].iloc[0]}\"\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated data\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "print(synthetic_df.head()) # Print first few examples to check\n",
    "\n",
    "# (Optional) Save DataFrame to CSV\n",
    "synthetic_df.to_csv(\"synthetic_igala_english.csv3\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m adjective \u001b[38;5;241m=\u001b[39m get_random_word(data, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbJJ\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Adjective\u001b[39;00m\n\u001b[0;32m     31\u001b[0m verb \u001b[38;5;241m=\u001b[39m get_random_word(data, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbVB\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Verb\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m noun2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_random_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mbNN\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Second noun\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Ensure we found valid words\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noun1 \u001b[38;5;129;01mand\u001b[39;00m noun2 \u001b[38;5;129;01mand\u001b[39;00m adjective \u001b[38;5;129;01mand\u001b[39;00m verb:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Example: \"big stone\" or \"man gives stone\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m, in \u001b[0;36mget_random_word\u001b[1;34m(df, pos_tag)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_random_word\u001b[39m(df, pos_tag):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets a random word from the DataFrame matching the given POS tag.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIgala\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m words:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(words)\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\frame.py:3496\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3494\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3498\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3499\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3500\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\frame.py:3551\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3549\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   3550\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 3551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\generic.py:3716\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3708\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   3709\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3710\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3711\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3714\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3715\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3717\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\generic.py:3703\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3699\u001b[0m nv\u001b[38;5;241m.\u001b[39mvalidate_take((), kwargs)\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3703\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   3705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    887\u001b[0m indexer \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    888\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(indexer\u001b[38;5;241m.\u001b[39mstart, indexer\u001b[38;5;241m.\u001b[39mstop, indexer\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m)\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masanyarray(indexer, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp)\n\u001b[0;32m    891\u001b[0m )\n\u001b[0;32m    893\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[axis]\n\u001b[1;32m--> 894\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    898\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    899\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m     consolidate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    903\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:291\u001b[0m, in \u001b[0;36mmaybe_convert_indices\u001b[1;34m(indices, n, verify)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify:\n\u001b[0;32m    290\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (indices \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n) \u001b[38;5;241m|\u001b[39m (indices \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n",
      "File \u001b[1;32mc:\\Users\\OWNER\\anaconda3\\envs\\gpt_env\\lib\\site-packages\\numpy\\core\\_methods.py:54\u001b[0m, in \u001b[0;36m_any\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_any\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"igala_updated_with_pos.csv\")\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_random_word(df, pos_tag):\n",
    "    \"\"\"Gets a random word from the DataFrame matching the given POS tag.\"\"\"\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    \"\"\"Gets the English translation of a given Igala word.\"\"\"\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# --- Generate Synthetic Data ---\n",
    "num_samples = 1000  # Control how many phrases you want to generate\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    noun1 = get_random_word(data, r\"\\bNN\\b\")  # First noun\n",
    "    adjective = get_random_word(data, r\"\\bJJ\\b\")  # Adjective\n",
    "    verb = get_random_word(data, r\"\\bVB\\b\")  # Verb\n",
    "    noun2 = get_random_word(data, r\"\\bNN\\b\")  # Second noun\n",
    "\n",
    "    # Ensure we found valid words\n",
    "    if noun1 and noun2 and adjective and verb:\n",
    "        # Example: \"big stone\" or \"man gives stone\"\n",
    "        igala_phrase = f\"{adjective} {noun1} {verb} {noun2}\"\n",
    "        english_phrase = f\"The {get_english_translation(data, adjective)} {get_english_translation(data, noun1)} {get_english_translation(data, verb)} the {get_english_translation(data, noun2)}\"\n",
    "        \n",
    "        # Adding more structure and coherence\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated data\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df.to_csv(\"synthetic_igala_english.csv3\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Improved Phrase Construction ---\n",
    "num_samples = 100\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    adjective = get_random_word(data, r\"\\bJJ\\b\")\n",
    "    noun1 = get_random_word(data, r\"\\bNN\\b\")\n",
    "    verb = get_random_word(data, r\"\\bVB\\b\")\n",
    "    noun2 = get_random_word(data, r\"\\bNN\\b\")\n",
    "\n",
    "    # Ensure we found valid words and apply more logical pairings\n",
    "    if noun1 and noun2 and adjective and verb:\n",
    "        if random.choice([True, False]):\n",
    "            # Adjective + Noun + Verb + Noun\n",
    "            igala_phrase = f\"{adjective} {noun1} {verb} {noun2}\"\n",
    "            english_phrase = f\"The {get_english_translation(data, adjective)} {get_english_translation(data, noun1)} {get_english_translation(data, verb)} the {get_english_translation(data, noun2)}\"\n",
    "        else:\n",
    "            # Noun + Verb + Noun\n",
    "            igala_phrase = f\"{noun1} {verb} {noun2}\"\n",
    "            english_phrase = f\"The {get_english_translation(data, noun1)} {get_english_translation(data, verb)} the {get_english_translation(data, noun2)}\"\n",
    "\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated data\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few sample phrases\n",
    "synthetic_df.head(10)\n",
    "synthetic_df.to_csv(\"synthetic_igala_english_100.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synthetic_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msynthetic_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic_igala_english_3000.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'synthetic_df' is not defined"
     ]
    }
   ],
   "source": [
    "synthetic_df.to_csv(\"synthetic_igala_english_3000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ábàìló dẹ́ d'ùbí úchẹ́-oko</td>\n",
       "      <td>The unreasonable available participate the agr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>édíọ́ ọ̀fọ́ kójì ànyí</td>\n",
       "      <td>The vocal justice replace the laughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jẹ̄ ọ́maye m̀ẹ́jú ọ́dọ̀</td>\n",
       "      <td>The enable sister identify the brick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lógwọ́ anẹ́ gbà ódú</td>\n",
       "      <td>The infectious p.m. give the name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gọ́ dọ́mọ ójì</td>\n",
       "      <td>The tour attempt the perception</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>ólí ẹ̀dọ̀ du éfúù</td>\n",
       "      <td>The Catholic attention take the mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>Àtá néjú ọ̀gẹ̀chá</td>\n",
       "      <td>The father believe the fact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>ñọ́ yọ du</td>\n",
       "      <td>The gather save the take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>òdè lé gbẹ́</td>\n",
       "      <td>The example beyond the okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>ẹ̀kẹ́lā Ójúwó lí  ẹ́rẹ́-únyí</td>\n",
       "      <td>The ninth Mount see the foundation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Igala  \\\n",
       "0        ábàìló dẹ́ d'ùbí úchẹ́-oko   \n",
       "1             édíọ́ ọ̀fọ́ kójì ànyí   \n",
       "2           jẹ̄ ọ́maye m̀ẹ́jú ọ́dọ̀   \n",
       "3              lógwọ́ anẹ́ gbà ódú    \n",
       "4                     gọ́ dọ́mọ ójì   \n",
       "...                             ...   \n",
       "19995             ólí ẹ̀dọ̀ du éfúù   \n",
       "19996             Àtá néjú ọ̀gẹ̀chá   \n",
       "19997                     ñọ́ yọ du   \n",
       "19998                   òdè lé gbẹ́   \n",
       "19999  ẹ̀kẹ́lā Ójúwó lí  ẹ́rẹ́-únyí   \n",
       "\n",
       "                                                 English  \n",
       "0      The unreasonable available participate the agr...  \n",
       "1                 The vocal justice replace the laughter  \n",
       "2                   The enable sister identify the brick  \n",
       "3                      The infectious p.m. give the name  \n",
       "4                        The tour attempt the perception  \n",
       "...                                                  ...  \n",
       "19995               The Catholic attention take the mind  \n",
       "19996                        The father believe the fact  \n",
       "19997                           The gather save the take  \n",
       "19998                        The example beyond the okay  \n",
       "19999                 The ninth Mount see the foundation  \n",
       "\n",
       "[19999 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('synthetic_igala_english_20000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "19995    False\n",
       "19996    False\n",
       "19997    False\n",
       "19998    False\n",
       "19999    False\n",
       "Length: 20000, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

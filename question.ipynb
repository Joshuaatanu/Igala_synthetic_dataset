{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why àbímọ́tọ gbà káálọ́?</td>\n",
       "      <td>Why does the children give hypothetical?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how údūdẹ́lè já ágọ́?</td>\n",
       "      <td>How does the standard grate coin?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>where ọ̀lùgbè là lọ́lá?</td>\n",
       "      <td>Where does the suspense buy efficient?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who àwúyà D’ẹ́jú ájọ?</td>\n",
       "      <td>Who does the waste Look global?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why íbe  já ígbélé?</td>\n",
       "      <td>Why does the mind grate ancient?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>where le ché hì?</td>\n",
       "      <td>Where does the somehow dealing cook?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>when ùkpẹ̄ lìá  káà?</td>\n",
       "      <td>When does the chapter come occasional?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when du lí dẹ́gbà?</td>\n",
       "      <td>When does the take see likely?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>who àgbà m̀ẹ́jú àñọ́lá?</td>\n",
       "      <td>Who does the chin identify physical?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>where ùfẹ̀dọ̀ gbá ẹ́já?</td>\n",
       "      <td>Where does the affection built military?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Igala                                   English\n",
       "0  why àbímọ́tọ gbà káálọ́?  Why does the children give hypothetical?\n",
       "1     how údūdẹ́lè já ágọ́?         How does the standard grate coin?\n",
       "2   where ọ̀lùgbè là lọ́lá?    Where does the suspense buy efficient?\n",
       "3     who àwúyà D’ẹ́jú ájọ?           Who does the waste Look global?\n",
       "4       why íbe  já ígbélé?          Why does the mind grate ancient?\n",
       "5          where le ché hì?      Where does the somehow dealing cook?\n",
       "6      when ùkpẹ̄ lìá  káà?    When does the chapter come occasional?\n",
       "7        when du lí dẹ́gbà?            When does the take see likely?\n",
       "8   who àgbà m̀ẹ́jú àñọ́lá?      Who does the chin identify physical?\n",
       "9   where ùfẹ̀dọ̀ gbá ẹ́já?  Where does the affection built military?"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Generate Questions ---\n",
    "num_samples = 1000\n",
    "synthetic_data = []\n",
    "\n",
    "interrogatives = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"]\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    interrogative = random.choice(interrogatives)\n",
    "    noun = get_random_word(data, r\"\\bNN\\b\")  # Noun\n",
    "    verb = get_random_word(data, r\"\\bVB\\b\")  # Verb\n",
    "    adjective = get_random_word(data, r\"\\bJJ\\b\")  # Adjective (optional)\n",
    "\n",
    "    # Ensure valid words were found\n",
    "    if noun and verb:\n",
    "        # Form the question\n",
    "        if adjective:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb} {adjective}?\"\n",
    "            english_phrase = f\"{interrogative.capitalize()} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)} {get_english_translation(data, adjective)}?\"\n",
    "        else:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb}?\"\n",
    "            english_phrase = f\"{interrogative.capitalize()} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)}?\"\n",
    "\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few sample questions\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what ọ́maye kójì lógwọ́?</td>\n",
       "      <td>what does the sister replace infectious?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when ítélénì  lìá  n’ùyọ̀?</td>\n",
       "      <td>when does the tray come unhappy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>who ọ̀ọ́nà chí ẹ́nẹ́-káà?</td>\n",
       "      <td>who does the canal appoint individual?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where íkwúù lìá òfeje-í?</td>\n",
       "      <td>where does the line arrive recent?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who ẹ̀bẹ́lẹ́ abalẹ d’éjìjì?</td>\n",
       "      <td>who does the chip be nervous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>when kpẹ́jà che òbògo?</td>\n",
       "      <td>when does the fishing is stupid?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>who ìtíchà enẹ Báíbùlù?</td>\n",
       "      <td>who does the instructor have Bible?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>who ẹ̀gwélā dágba ọ̀gbọ́lọ́?</td>\n",
       "      <td>who does the nineteen may be crucial?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>what áji jẹ́ fọ̀?</td>\n",
       "      <td>what does the stream convenient cripple?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>when Bàbá d'ùbí bibi?</td>\n",
       "      <td>when does the daddy participate undesirable?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Igala                                       English\n",
       "0      what ọ́maye kójì lógwọ́?      what does the sister replace infectious?\n",
       "1    when ítélénì  lìá  n’ùyọ̀?              when does the tray come unhappy?\n",
       "2     who ọ̀ọ́nà chí ẹ́nẹ́-káà?        who does the canal appoint individual?\n",
       "3      where íkwúù lìá òfeje-í?            where does the line arrive recent?\n",
       "4   who ẹ̀bẹ́lẹ́ abalẹ d’éjìjì?                 who does the chip be nervous?\n",
       "5        when kpẹ́jà che òbògo?              when does the fishing is stupid?\n",
       "6       who ìtíchà enẹ Báíbùlù?           who does the instructor have Bible?\n",
       "7  who ẹ̀gwélā dágba ọ̀gbọ́lọ́?         who does the nineteen may be crucial?\n",
       "8             what áji jẹ́ fọ̀?      what does the stream convenient cripple?\n",
       "9         when Bàbá d'ùbí bibi?  when does the daddy participate undesirable?"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Generate Questions ---\n",
    "num_samples = 1000\n",
    "synthetic_data = []\n",
    "\n",
    "# Interrogatives list (kept in English)\n",
    "interrogatives = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\"]\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    interrogative = random.choice(interrogatives)\n",
    "    noun = get_random_word(data, r\"\\bNN\\b\")  # Noun\n",
    "    verb = get_random_word(data, r\"\\bVB\\b\")  # Verb\n",
    "    adjective = get_random_word(data, r\"\\bJJ\\b\")  # Adjective (optional)\n",
    "\n",
    "    # Ensure valid words were found\n",
    "    if noun and verb:\n",
    "        # Form the Igala question structure\n",
    "        if adjective:\n",
    "            igala_phrase = f\"{noun} {verb} {adjective}?\"\n",
    "            english_phrase = f\"{interrogative} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)} {get_english_translation(data, adjective)}?\"\n",
    "        else:\n",
    "            igala_phrase = f\"{noun} {verb}?\"\n",
    "            english_phrase = f\"{interrogative} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)}?\"\n",
    "\n",
    "        # Combine interrogative and question structure\n",
    "        synthetic_data.append(\n",
    "            [f\"{interrogative} {igala_phrase}\", english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few sample questions\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kù  tọ́ dú dàbì  neke?</td>\n",
       "      <td>who does the resultant go back the can?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>énẹ́ iji-úná néjú neke?</td>\n",
       "      <td>which does the log believe the can?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>úgbò Ọ́wọ́ gbà jẹjú?</td>\n",
       "      <td>Where does the Hand give the assemble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ugbo áhímú lí  éjénẹ́ẹ̀?</td>\n",
       "      <td>where does the psychiatric see the popular?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>énẹ́ gbómù enẹ jálíí?</td>\n",
       "      <td>which does the respond have the obviously?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ugbo wúùùlù ñọ́rú gbẹ́jú?</td>\n",
       "      <td>where does the dizzy settle the fraudulent?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kéee  gbà ñọ́rú mà?</td>\n",
       "      <td>why does the give settle the they?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kéee  hìẹ̀dọ̀ wánẹ́ẹ̀ bàkú  gbẹ́gā?</td>\n",
       "      <td>why does the calm must be the dynamic?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kalí búla tā ẹ̀nyọ̀?</td>\n",
       "      <td>Which does the crash impose the good?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ugbo n’úká gbà ẹ̀gwejì?</td>\n",
       "      <td>where does the attack give the Double?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Igala  \\\n",
       "0               kù  tọ́ dú dàbì  neke?   \n",
       "1             énẹ́ iji-úná néjú neke?   \n",
       "2                 úgbò Ọ́wọ́ gbà jẹjú?   \n",
       "3             ugbo áhímú lí  éjénẹ́ẹ̀?   \n",
       "4                énẹ́ gbómù enẹ jálíí?   \n",
       "5            ugbo wúùùlù ñọ́rú gbẹ́jú?   \n",
       "6                  kéee  gbà ñọ́rú mà?   \n",
       "7  kéee  hìẹ̀dọ̀ wánẹ́ẹ̀ bàkú  gbẹ́gā?   \n",
       "8                 kalí búla tā ẹ̀nyọ̀?   \n",
       "9              ugbo n’úká gbà ẹ̀gwejì?   \n",
       "\n",
       "                                       English  \n",
       "0      who does the resultant go back the can?  \n",
       "1          which does the log believe the can?  \n",
       "2       Where does the Hand give the assemble?  \n",
       "3  where does the psychiatric see the popular?  \n",
       "4   which does the respond have the obviously?  \n",
       "5  where does the dizzy settle the fraudulent?  \n",
       "6           why does the give settle the they?  \n",
       "7       why does the calm must be the dynamic?  \n",
       "8        Which does the crash impose the good?  \n",
       "9       where does the attack give the Double?  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Extract Interrogatives ---\n",
    "interrogative_tags = [\"WP\", \"WRB\", \"WDT\"]\n",
    "interrogatives_df = data[data[\"POS\"].isin(interrogative_tags)]\n",
    "\n",
    "# --- Generate Questions ---\n",
    "num_samples = 1000\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    interrogative = get_random_word(\n",
    "        interrogatives_df, \"|\".join(interrogative_tags))\n",
    "\n",
    "    noun = get_random_word(data, r\"\\bNN\\b\")  # Noun\n",
    "    verb = get_random_word(data, r\"\\bVB\\b\")  # Verb\n",
    "    adjective = get_random_word(data, r\"\\bJJ\\b\")  # Adjective (optional)\n",
    "\n",
    "    # Ensure valid words were found\n",
    "    if interrogative and noun and verb:\n",
    "        # Construct Igala phrase\n",
    "        if adjective:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb} {adjective}?\"\n",
    "        else:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb}?\"\n",
    "\n",
    "        # Construct English phrase\n",
    "        if adjective:\n",
    "            english_phrase = f\"{get_english_translation(data, interrogative)} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)} the {get_english_translation(data, adjective)}?\"\n",
    "        else:\n",
    "            english_phrase = f\"{get_english_translation(data, interrogative)} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)}?\"\n",
    "\n",
    "        # Append to synthetic data\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few sample questions\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\AppData\\Local\\Temp\\ipykernel_4324\\3336952144.py:19: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ugbo úgbá kọ̀ káálọ́?</td>\n",
       "      <td>where does the basin opposed the hypothetical?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kéee  lù lèkwú  únyí?</td>\n",
       "      <td>why does the smell will die the house?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kéee  kpọ́tii ché ágbojì?</td>\n",
       "      <td>why does the thick made the stronger?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ugbo Ẹ̀pìlì kpégā ígbélẹ́?</td>\n",
       "      <td>where does the April reminded the historical?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ábú ágbee rá ọjó?</td>\n",
       "      <td>how does the injury run the seasonal?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kálí  gwúné kẹ̀dọ̀nó ìgbẹ̀lẹ́ ?</td>\n",
       "      <td>which does the accuse hoping the young girl?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kù  úñà nẹ d’éjìjì?</td>\n",
       "      <td>who does the seat own the nervous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>édú ch’ókpò égbánẹ́ẹ̀ jẹjú?</td>\n",
       "      <td>Function does the afraid sweeping the assemble?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>édú ẹnẹ́káàlù d’ọ́ d’éjìjì?</td>\n",
       "      <td>Function does the speaker embodied the nervous?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kéee  ùkpẹ̄ jẹ́nyú jó?</td>\n",
       "      <td>why does the chapter assure the burn?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Igala  \\\n",
       "0            ugbo úgbá kọ̀ káálọ́?   \n",
       "1            kéee  lù lèkwú  únyí?   \n",
       "2       kéee  kpọ́tii ché ágbojì?   \n",
       "3       ugbo Ẹ̀pìlì kpégā ígbélẹ́?   \n",
       "4                ábú ágbee rá ọjó?   \n",
       "5  kálí  gwúné kẹ̀dọ̀nó ìgbẹ̀lẹ́ ?   \n",
       "6              kù  úñà nẹ d’éjìjì?   \n",
       "7      édú ch’ókpò égbánẹ́ẹ̀ jẹjú?   \n",
       "8      édú ẹnẹ́káàlù d’ọ́ d’éjìjì?   \n",
       "9           kéee  ùkpẹ̄ jẹ́nyú jó?   \n",
       "\n",
       "                                           English  \n",
       "0   where does the basin opposed the hypothetical?  \n",
       "1           why does the smell will die the house?  \n",
       "2            why does the thick made the stronger?  \n",
       "3    where does the April reminded the historical?  \n",
       "4            how does the injury run the seasonal?  \n",
       "5     which does the accuse hoping the young girl?  \n",
       "6               who does the seat own the nervous?  \n",
       "7  Function does the afraid sweeping the assemble?  \n",
       "8  Function does the speaker embodied the nervous?  \n",
       "9            why does the chapter assure the burn?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Define themes and corresponding word filters\n",
    "themes = {\n",
    "    \"Family\": {\"nouns\": [\"NN\", \"NNS\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]},\n",
    "    \"Nature\": {\"nouns\": [\"NN\", \"NNS\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]},\n",
    "    \"Daily Activities\": {\"nouns\": [\"NN\", \"NNS\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]},\n",
    "    \"Greetings\": {\"nouns\": [\"NN\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]},\n",
    "    \"Classroom\": {\"nouns\": [\"NN\", \"NNS\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]},\n",
    "    \"Market\": {\"nouns\": [\"NN\", \"NNS\"], \"verbs\": [\"VB\", \"VBP\"], \"adjectives\": [\"JJ\"]}\n",
    "}\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Choose a theme\n",
    "# You can change this to \"Nature\", \"Daily Activities\", etc.\n",
    "selected_theme = \"Greetings\"\n",
    "\n",
    "# Filter words based on the selected theme\n",
    "noun_pos_tags = themes[selected_theme][\"nouns\"]\n",
    "verb_pos_tags = themes[selected_theme][\"verbs\"]\n",
    "adjective_pos_tags = themes[selected_theme][\"adjectives\"]\n",
    "\n",
    "# Generate phrases for the selected theme\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    interrogative = get_random_word(data, r\"\\b(WP|WRB|WDT)\\b\")\n",
    "    noun = get_random_word(data, \"|\".join(noun_pos_tags))\n",
    "    verb = get_random_word(data, \"|\".join(verb_pos_tags))\n",
    "    adjective = get_random_word(data, \"|\".join(adjective_pos_tags))\n",
    "\n",
    "    if interrogative and noun and verb:\n",
    "        if adjective:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb} {adjective}?\"\n",
    "        else:\n",
    "            igala_phrase = f\"{interrogative} {noun} {verb}?\"\n",
    "\n",
    "        english_phrase = f\"{get_english_translation(data, interrogative)} does the {get_english_translation(data, noun)} {get_english_translation(data, verb)}\"\n",
    "        if adjective:\n",
    "            english_phrase += f\" the {get_english_translation(data, adjective)}?\"\n",
    "        else:\n",
    "            english_phrase += \"?\"\n",
    "\n",
    "        synthetic_data.append([igala_phrase, english_phrase])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few sample questions\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "     ---------------------------------------- 0.0/125.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 125.2/125.2 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp310-cp310-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\owner\\anaconda3\\envs\\gpt_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.7.6-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/12.1 MB 5.4 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.5/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/12.1 MB 4.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.9/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/12.1 MB 4.7 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 4.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 5.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.2/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.7/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.9/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 4.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.3/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.6/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.1/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.5/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.8/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.8/12.1 MB 4.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.1/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.6/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.3/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.4/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.6/12.1 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.7/12.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.9/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.9/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.2/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.3/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.6/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.8/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.3/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.8/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.3/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "   ---------------------------------------- 0.0/423.9 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 225.3/423.9 kB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 423.9/423.9 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.20.1-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.9 MB 8.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/1.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/1.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/1.9 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.2/1.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.5/1.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.7/1.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 286.7/481.9 kB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 481.9/481.9 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.5 MB 6.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.3/1.5 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.2 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/6.6 MB 6.9 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.4/6.6 MB 7.6 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.6/6.6 MB 6.5 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.2/6.6 MB 7.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.5/6.6 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.0/6.6 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.6/6.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.5/6.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.6 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.5/6.6 MB 7.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.8/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.8/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.0/6.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.3/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.7/6.6 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 6.9 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/5.4 MB 13.0 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.2/5.4 MB 13.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.5/5.4 MB 4.3 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 6.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.0/5.4 MB 5.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.3/5.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.9/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.3/5.4 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 2.6/5.4 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.8/5.4 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.8/5.4 MB 7.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.8/5.4 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.3/5.4 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.6/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 3.8/5.4 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.1/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.4/5.4 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.6/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.8/5.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "   ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 61.2/61.2 kB ? eta 0:00:00\n",
      "Downloading marisa_trie-1.2.0-cp310-cp310-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.4/152.4 kB 8.9 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.8.2 pydantic-core-2.20.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1553972540.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python -m spacy download en_core_web_sm\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who ADJ the NOUN ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where nẹ́ the NOUN VERB ADJ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the ọ́kọ àkpàtì ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the ábọ́-ọ́gwù ìchèkpúlù ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the édìbò búla ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who ADJ the NOUN ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does the ánẹ́ẹ̀ ígbélí ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Where gwá the NOUN VERB ADJ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where lé the NOUN VERB ADJ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does the éfúù àmẹ̀ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Igala  \\\n",
       "0                    Who ADJ the NOUN ?   \n",
       "1         Where nẹ́ the NOUN VERB ADJ ?   \n",
       "2           What does the ọ́kọ àkpàtì ?   \n",
       "3  What does the ábọ́-ọ́gwù ìchèkpúlù ?   \n",
       "4            What does the édìbò búla ?   \n",
       "5                    Who ADJ the NOUN ?   \n",
       "6         What does the ánẹ́ẹ̀ ígbélí ?   \n",
       "7         Where gwá the NOUN VERB ADJ ?   \n",
       "8          Where lé the NOUN VERB ADJ ?   \n",
       "9             What does the éfúù àmẹ̀ ?   \n",
       "\n",
       "                                  English  \n",
       "0  Translate the sentence to English here  \n",
       "1  Translate the sentence to English here  \n",
       "2  Translate the sentence to English here  \n",
       "3  Translate the sentence to English here  \n",
       "4  Translate the sentence to English here  \n",
       "5  Translate the sentence to English here  \n",
       "6  Translate the sentence to English here  \n",
       "7  Translate the sentence to English here  \n",
       "8  Translate the sentence to English here  \n",
       "9  Translate the sentence to English here  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English model for parsing and tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Helper function to get words based on POS\n",
    "\n",
    "\n",
    "def get_word_by_pos(df, pos):\n",
    "    words = df[df[\"POS\"].str.contains(pos, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate sentences using templates and POS information\n",
    "\n",
    "\n",
    "def generate_sentence(template, words_dict):\n",
    "    doc = nlp(template)\n",
    "    sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in words_dict:\n",
    "            word = get_word_by_pos(data, words_dict[token.pos_])\n",
    "            if word:\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentence.append(token.text)\n",
    "        else:\n",
    "            sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "# Define templates and corresponding POS tags\n",
    "templates = [\n",
    "    \"What does the NOUN VERB?\",\n",
    "    \"Who ADJ the NOUN?\",\n",
    "    \"Where does the NOUN VERB ADJ?\"\n",
    "]\n",
    "\n",
    "# Mapping POS tags in the template to the dataset POS tags\n",
    "words_dict = {\n",
    "    'NOUN': 'NN',\n",
    "    'VERB': 'VB',\n",
    "    'ADJ': 'JJ'\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    template = random.choice(templates)\n",
    "    igala_sentence = generate_sentence(template, words_dict)\n",
    "\n",
    "    # Here, we assume a similar template-based approach for English translation\n",
    "    # You may need to translate Igala words to English after generation\n",
    "    english_translation = \"Translate the sentence to English here\"\n",
    "\n",
    "    synthetic_data.append([igala_sentence, english_translation])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few samples\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who lotoo the ójáanẹ̀ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who àbígbo the tílílí ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where does the ímótò-ejọ́mi dúgbo jì ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the égbìgbì kà ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where does the éjú chẹ́ gbó ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What does the àmíbāálú che ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Where does the áchíkwúù ch’ámínáà étító ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who ọ́gbọ́n the ónúgò ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where does the ẹ̀kọ́-ùmà ténẹ́ ẹ̀kpíkpà ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Where does the ìgbẹ̀lẹ́  gbà lìlẹ̀ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Igala  \\\n",
       "0                    Who lotoo the ójáanẹ̀ ?   \n",
       "1                    Who àbígbo the tílílí ?   \n",
       "2     Where does the ímótò-ejọ́mi dúgbo jì ?   \n",
       "3                 What does the égbìgbì kà ?   \n",
       "4              Where does the éjú chẹ́ gbó ?   \n",
       "5               What does the àmíbāálú che ?   \n",
       "6  Where does the áchíkwúù ch’ámínáà étító ?   \n",
       "7                    Who ọ́gbọ́n the ónúgò ?   \n",
       "8  Where does the ẹ̀kọ́-ùmà ténẹ́ ẹ̀kpíkpà ?   \n",
       "9       Where does the ìgbẹ̀lẹ́  gbà lìlẹ̀ ?   \n",
       "\n",
       "                                  English  \n",
       "0  Translate the sentence to English here  \n",
       "1  Translate the sentence to English here  \n",
       "2  Translate the sentence to English here  \n",
       "3  Translate the sentence to English here  \n",
       "4  Translate the sentence to English here  \n",
       "5  Translate the sentence to English here  \n",
       "6  Translate the sentence to English here  \n",
       "7  Translate the sentence to English here  \n",
       "8  Translate the sentence to English here  \n",
       "9  Translate the sentence to English here  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English model for parsing and tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Helper function to get words based on POS\n",
    "\n",
    "\n",
    "def get_word_by_pos(df, pos):\n",
    "    words = df[df[\"POS\"].str.contains(pos, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate sentences using templates and POS information\n",
    "\n",
    "\n",
    "def generate_sentence(template, words_dict):\n",
    "    doc = nlp(template)\n",
    "    sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.upper() in words_dict:\n",
    "            word = get_word_by_pos(data, words_dict[token.text.upper()])\n",
    "            if word:\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentence.append(token.text)\n",
    "        else:\n",
    "            sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "# Define templates and corresponding POS tags\n",
    "templates = [\n",
    "    \"What does the NOUN VERB?\",\n",
    "    \"Who ADJ the NOUN?\",\n",
    "    \"Where does the NOUN VERB ADJ?\"\n",
    "]\n",
    "\n",
    "# Mapping POS tags in the template to the dataset POS tags\n",
    "words_dict = {\n",
    "    'NOUN': 'NN',\n",
    "    'VERB': 'VB',\n",
    "    'ADJ': 'JJ'\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    template = random.choice(templates)\n",
    "    igala_sentence = generate_sentence(template, words_dict)\n",
    "\n",
    "    # Here, you may need to translate Igala words to English after generation\n",
    "    english_translation = \"Translate the sentence to English here\"\n",
    "\n",
    "    synthetic_data.append([igala_sentence, english_translation])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few samples\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who mà the mà ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who òfeje-í the NOUN ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who ọ́kọ́ the ọ́kọ́ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the d’ọ́jọ́ jó ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the ódú dú ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Where does the áyé VERB ADJ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Who kpákáá the kpá ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Where does the ọ́màjùwẹ mà mà ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where does the ẹ́gẹ́ VERB ADJ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who ọ́jọ́-ọ́jọ́ the jó ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Igala                                 English\n",
       "0                  Who mà the mà ?  Translate the sentence to English here\n",
       "1           Who òfeje-í the NOUN ?  Translate the sentence to English here\n",
       "2            Who ọ́kọ́ the ọ́kọ́ ?  Translate the sentence to English here\n",
       "3       What does the d’ọ́jọ́ jó ?  Translate the sentence to English here\n",
       "4           What does the ódú dú ?  Translate the sentence to English here\n",
       "5    Where does the áyé VERB ADJ ?  Translate the sentence to English here\n",
       "6             Who kpákáá the kpá ?  Translate the sentence to English here\n",
       "7  Where does the ọ́màjùwẹ mà mà ?  Translate the sentence to English here\n",
       "8  Where does the ẹ́gẹ́ VERB ADJ ?  Translate the sentence to English here\n",
       "9         Who ọ́jọ́-ọ́jọ́ the jó ?  Translate the sentence to English here"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English model for parsing and tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Helper function to get words based on POS\n",
    "\n",
    "\n",
    "def get_word_by_pos(df, pos, context=None):\n",
    "    words = df[df[\"POS\"].str.contains(pos, na=False)][\"Igala\"].tolist()\n",
    "    # Filter further by context if provided\n",
    "    if context:\n",
    "        words = [word for word in words if word in context]\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate sentences using templates and POS information\n",
    "\n",
    "\n",
    "def generate_sentence(template, words_dict):\n",
    "    doc = nlp(template)\n",
    "    sentence = []\n",
    "    context = None\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.upper() in words_dict:\n",
    "            word = get_word_by_pos(\n",
    "                data, words_dict[token.text.upper()], context)\n",
    "            if word:\n",
    "                sentence.append(word)\n",
    "                context = word  # Update context\n",
    "            else:\n",
    "                sentence.append(token.text)\n",
    "        else:\n",
    "            sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "# Define templates and corresponding POS tags\n",
    "templates = [\n",
    "    \"What does the NOUN VERB?\",\n",
    "    \"Who ADJ the NOUN?\",\n",
    "    \"Where does the NOUN VERB ADJ?\"\n",
    "]\n",
    "\n",
    "# Mapping POS tags in the template to the dataset POS tags\n",
    "words_dict = {\n",
    "    'NOUN': 'NN',\n",
    "    'VERB': 'VB',\n",
    "    'ADJ': 'JJ'\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    template = random.choice(templates)\n",
    "    igala_sentence = generate_sentence(template, words_dict)\n",
    "\n",
    "    # Translate the Igala sentence into English\n",
    "    # Update with actual translation logic\n",
    "    english_translation = \"Translate the sentence to English here\"\n",
    "\n",
    "    synthetic_data.append([igala_sentence, english_translation])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few samples\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who kẹ̀chùù the gbéjú ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the gbà gbá ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who kpé the àwọ̀nyí ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the ígẹ̀dẹ́ kà ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where does the jálí já ẹ̀gwù ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Who ẹ́kẹ́jì the li ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does the k’úkwú jẹ́ dú ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What does the àmómi égbánẹ́ẹ̀ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Who bíẹ́nẹ́-bíẹ́nẹ the àmìlẹ̀ ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Where does the bílú ùgbéjú gbẹ́gā ?</td>\n",
       "      <td>Translate the sentence to English here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Igala                                 English\n",
       "0              Who kẹ̀chùù the gbéjú ?  Translate the sentence to English here\n",
       "1              What does the gbà gbá ?  Translate the sentence to English here\n",
       "2                Who kpé the àwọ̀nyí ?  Translate the sentence to English here\n",
       "3           What does the ígẹ̀dẹ́ kà ?  Translate the sentence to English here\n",
       "4      Where does the jálí já ẹ̀gwù ?  Translate the sentence to English here\n",
       "5                 Who ẹ́kẹ́jì the li ?  Translate the sentence to English here\n",
       "6        What does the k’úkwú jẹ́ dú ?  Translate the sentence to English here\n",
       "7      What does the àmómi égbánẹ́ẹ̀ ?  Translate the sentence to English here\n",
       "8      Who bíẹ́nẹ́-bíẹ́nẹ the àmìlẹ̀ ?  Translate the sentence to English here\n",
       "9  Where does the bílú ùgbéjú gbẹ́gā ?  Translate the sentence to English here"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English model for parsing and tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Helper function to get words based on POS\n",
    "\n",
    "\n",
    "def get_word_by_pos(df, pos):\n",
    "    words = df[df[\"POS\"].str.contains(pos, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Generate sentences using templates and POS information\n",
    "\n",
    "\n",
    "def generate_sentence(template, words_dict):\n",
    "    doc = nlp(template)\n",
    "    sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        token_upper = token.text.upper()\n",
    "        if token_upper in words_dict:\n",
    "            word = get_word_by_pos(data, words_dict[token_upper])\n",
    "            if word and word not in sentence:  # Ensure the word is unique within the sentence\n",
    "                sentence.append(word)\n",
    "            else:\n",
    "                sentence.append(f\"<missing {token_upper}>\")\n",
    "        else:\n",
    "            sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "\n",
    "# Define templates and corresponding POS tags\n",
    "templates = [\n",
    "    \"What does the NOUN VERB?\",\n",
    "    \"Who ADJ the NOUN?\",\n",
    "    \"Where does the NOUN VERB ADJ?\"\n",
    "]\n",
    "\n",
    "# Mapping POS tags in the template to the dataset POS tags\n",
    "words_dict = {\n",
    "    'NOUN': 'NN',\n",
    "    'VERB': 'VB',\n",
    "    'ADJ': 'JJ'\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    template = random.choice(templates)\n",
    "    igala_sentence = generate_sentence(template, words_dict)\n",
    "\n",
    "    # Placeholder for English translation (to be implemented)\n",
    "    english_translation = \"Translate the sentence to English here\"\n",
    "\n",
    "    synthetic_data.append([igala_sentence, english_translation])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few samples\n",
    "synthetic_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Igala</th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who bílẹ́wá the jẹ́  ?</td>\n",
       "      <td>Who modern the agree ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where does the bà d'ùbí dárú ?</td>\n",
       "      <td>Where does the curve participated separate ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where does the ẹgwa èbíè nẹ́ Ígáláà ?</td>\n",
       "      <td>Where does the seventeen constructing indigeno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the ọ́kọ nẹ́ ?</td>\n",
       "      <td>What does the husband hiring ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Where does the àdárú kpíjèlè gbẹ́gbẹ́dẹ́ ?</td>\n",
       "      <td>Where does the separation freezing customary ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Where does the àbímọ́tọ d'ùbí jí ?</td>\n",
       "      <td>Where does the family participate unified ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Where does the Ọ́jọ́ chánẹ́ éfù ?</td>\n",
       "      <td>Where does the God started internal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who ìgbẹ̀lẹ́  the òchìkapa  ?</td>\n",
       "      <td>Who young girl the rice ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Where does the ójáanẹ́ tẹ̄ e̩ko̩ ?</td>\n",
       "      <td>Where does the State defined full ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Who kàkpọ́ọ́ the ñwu ?</td>\n",
       "      <td>Who substantial the itch ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Igala  \\\n",
       "0                      Who bílẹ́wá the jẹ́  ?   \n",
       "1              Where does the bà d'ùbí dárú ?   \n",
       "2       Where does the ẹgwa èbíè nẹ́ Ígáláà ?   \n",
       "3                    What does the ọ́kọ nẹ́ ?   \n",
       "4  Where does the àdárú kpíjèlè gbẹ́gbẹ́dẹ́ ?   \n",
       "5          Where does the àbímọ́tọ d'ùbí jí ?   \n",
       "6           Where does the Ọ́jọ́ chánẹ́ éfù ?   \n",
       "7               Who ìgbẹ̀lẹ́  the òchìkapa  ?   \n",
       "8          Where does the ójáanẹ́ tẹ̄ e̩ko̩ ?   \n",
       "9                      Who kàkpọ́ọ́ the ñwu ?   \n",
       "\n",
       "                                             English  \n",
       "0                             Who modern the agree ?  \n",
       "1       Where does the curve participated separate ?  \n",
       "2  Where does the seventeen constructing indigeno...  \n",
       "3                     What does the husband hiring ?  \n",
       "4     Where does the separation freezing customary ?  \n",
       "5        Where does the family participate unified ?  \n",
       "6              Where does the God started internal ?  \n",
       "7                          Who young girl the rice ?  \n",
       "8                Where does the State defined full ?  \n",
       "9                         Who substantial the itch ?  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy English model for parsing and tagging\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# Helper function to get words based on POS\n",
    "\n",
    "\n",
    "def get_word_by_pos(df, pos):\n",
    "    words = df[df[\"POS\"].str.contains(pos, na=False)]\n",
    "    if not words.empty:\n",
    "        return words.sample(1).iloc[0]\n",
    "    return None\n",
    "\n",
    "# Generate sentences using templates and POS information\n",
    "\n",
    "\n",
    "def generate_sentence(template, words_dict):\n",
    "    doc = nlp(template)\n",
    "    igala_sentence = []\n",
    "    english_sentence = []\n",
    "\n",
    "    for token in doc:\n",
    "        token_upper = token.text.upper()\n",
    "        if token_upper in words_dict:\n",
    "            word_data = get_word_by_pos(data, words_dict[token_upper])\n",
    "            if word_data is not None:\n",
    "                igala_word = word_data['Igala']\n",
    "                english_word = word_data['English']\n",
    "                igala_sentence.append(igala_word)\n",
    "                english_sentence.append(english_word)\n",
    "            else:\n",
    "                igala_sentence.append(f\"<missing {token_upper}>\")\n",
    "                english_sentence.append(f\"<missing {token_upper}>\")\n",
    "        else:\n",
    "            igala_sentence.append(token.text)\n",
    "            english_sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(igala_sentence), \" \".join(english_sentence)\n",
    "\n",
    "\n",
    "# Define templates and corresponding POS tags\n",
    "templates = [\n",
    "    \"What does the NOUN VERB?\",\n",
    "    \"Who ADJ the NOUN?\",\n",
    "    \"Where does the NOUN VERB ADJ?\"\n",
    "]\n",
    "\n",
    "# Mapping POS tags in the template to the dataset POS tags\n",
    "words_dict = {\n",
    "    'NOUN': 'NN',\n",
    "    'VERB': 'VB',\n",
    "    'ADJ': 'JJ'\n",
    "}\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 10\n",
    "synthetic_data = []\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    template = random.choice(templates)\n",
    "    igala_sentence, english_translation = generate_sentence(\n",
    "        template, words_dict)\n",
    "\n",
    "    synthetic_data.append([igala_sentence, english_translation])\n",
    "\n",
    "# Create DataFrame from the generated questions\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Igala\", \"English\"])\n",
    "\n",
    "# Display a few samples\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gbà gbè íkọ́líkọ́ . Ó ní uná ákpata. dẹ̀ yọ álí. Ọ̀jọ̀ má dágba. Ṣùgbọ́n gbà kà dẹ̀. Ìtàn náà ọ̀pè ágọ́fó íkọ́líkọ́ .',\n",
       " 'The give went to the horror. There was a electric bridge. The configuration save the custom. The rain did may be. But the give say the configuration. The story ended with a formative horror.')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('igala_updated_with_pos.csv')\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "\n",
    "def get_random_word(df, pos_tag):\n",
    "    \"\"\"Gets a random word from the DataFrame matching the given POS tag.\"\"\"\n",
    "    words = df[df[\"POS\"].str.contains(pos_tag, na=False)][\"Igala\"].tolist()\n",
    "    if words:\n",
    "        return random.choice(words)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_english_translation(df, igala_word):\n",
    "    \"\"\"Gets the English translation of a given Igala word.\"\"\"\n",
    "    translation = df[df[\"Igala\"] == igala_word][\"English\"].tolist()\n",
    "    if translation:\n",
    "        return translation[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Generate Story Elements ---\n",
    "characters = [get_random_word(data, r\"\\bNN\\b\")\n",
    "              for _ in range(2)]  # Nouns for characters\n",
    "setting = get_random_word(data, r\"\\bNN\\b\")  # Noun for setting\n",
    "objects = [get_random_word(data, r\"\\bNN\\b\")\n",
    "           for _ in range(2)]  # Nouns for objects\n",
    "actions = [get_random_word(data, r\"\\bVB\\b\")\n",
    "           for _ in range(3)]  # Verbs for actions\n",
    "descriptions = [get_random_word(data, r\"\\bJJ\\b\")\n",
    "                for _ in range(2)]  # Adjectives for descriptions\n",
    "\n",
    "# --- Construct Story ---\n",
    "story = []\n",
    "\n",
    "# Beginning\n",
    "story.append(f\"{characters[0]} gbè {setting}.\")\n",
    "story.append(f\"Ó ní {descriptions[0]} {objects[0]}.\")\n",
    "\n",
    "# Middle\n",
    "story.append(f\"{characters[1]} {actions[0]} {objects[1]}.\")\n",
    "story.append(f\"Ọ̀jọ̀ má {actions[1]}.\")\n",
    "\n",
    "# End\n",
    "story.append(f\"Ṣùgbọ́n {characters[0]} {actions[2]} {characters[1]}.\")\n",
    "story.append(f\"Ìtàn náà ọ̀pè {descriptions[1]} {setting}.\")\n",
    "\n",
    "# Construct English translation\n",
    "english_story = []\n",
    "\n",
    "english_story.append(\n",
    "    f\"The {get_english_translation(data, characters[0])} went to the {get_english_translation(data, setting)}.\")\n",
    "english_story.append(\n",
    "    f\"There was a {get_english_translation(data, descriptions[0])} {get_english_translation(data, objects[0])}.\")\n",
    "english_story.append(\n",
    "    f\"The {get_english_translation(data, characters[1])} {get_english_translation(data, actions[0])} the {get_english_translation(data, objects[1])}.\")\n",
    "english_story.append(\n",
    "    f\"The rain did {get_english_translation(data, actions[1])}.\")\n",
    "english_story.append(\n",
    "    f\"But the {get_english_translation(data, characters[0])} {get_english_translation(data, actions[2])} the {get_english_translation(data, characters[1])}.\")\n",
    "english_story.append(\n",
    "    f\"The story ended with a {get_english_translation(data, descriptions[1])} {get_english_translation(data, setting)}.\")\n",
    "\n",
    "# Combine Igala and English story\n",
    "igala_story = \" \".join(story)\n",
    "english_story_text = \" \".join(english_story)\n",
    "\n",
    "# Output the stories\n",
    "igala_story, english_story_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
